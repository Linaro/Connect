---
categories:
- bkk19
description: Tutorial and Hands-on Accelerating AI Inference For Edge and IoT <br
  /> <br /> Applications involving voluminous data but needing low-latency computation
  and local feedback require that the computing be performed as close to the data
  source as possible --- often at the interface to the physical world. Communication
  constraints and the need for privacy also dictate the need for computing at the
  edge. Given the growth in such application scenarios and the recent advances in
  algorithms and techniques, machine learning and inference at the edge are unfolding
  and growing at a rapid pace. In support of these applications, a wide range of hardware
  (CPUs, GPUs, ASICs) is venturing farther away from the center, closer to the physical
  world. The resulting diversity in edge-computing hardware in terms of capabilities,
  architectures, and programming models poses several new challenges.<br /> At the
  edge, several applications often need to be scheduled concurrently or serially.
  Some applications may need to be run continuously, a few in anticipation of certain
  events, whereas others may need to be run when particular events occur, causing
  a need to unload other applications and dedicate resources to them. Situations may
  also warrant running applications in sandboxes for privacy, security, and resource
  allocation reasons. A future with heterogeneous edge hardware and multiple applications
  sharing the hardware and energy resources is imminent.<br /> <br /> The goal of
  this tutorial is to gather the community working in three broad areas and get feedback
  on :<br /> <br /> processing — artificial intelligence, computer vision, machine
  learning;<br /> <br /> management — parallel and distributed programming models
  for resource-constrained and domain-specific hardware, <br /> <br /> toolchain and
  runtimes - libraries (such as ArmNN), model compilers (such as TVM and Glow) and
  inference runtimes and formats such as ONNX <br /> <br /> The tutorial will provide
  a critically needed opportunity to discuss the current trends and issues, to share
  visions, and to present solutions. The first half will focus on hands on building
  and running a pre-trained neural network in PyTorch and TensorFlow to compile, execute
  and evaluate their empirical performance. <br /> <br /> Second half will focus on
  the Linaro Machine Intelligence SIG and the work currently being done with Linaro
  members around inference acceleration on Android and Linux devices. <br /> <br />
  Topics for the Hands on<br /> <br /> Introduction to Edge Inference using ONNX,
  Glow and TVM<br /> <br /> Hardware for Edge-computing and Machine Learning<br />
  CPU<br /> NPU<br /> DSP<br /> Compilation approaches for Deep Learning<br /> <br
  /> Project on building YOLO3 Object Detection<br /> <br /> Computer Vision at the
  Edge using ConvNet (MobileNet)<br />
image:
  featured: 'true'
  path: /assets/images/featured-images/bkk19/BKK19-408.png
session_attendee_num: '14'
session_id: BKK19-408
session_room: Session Room 1 (Lotus 1-2)
session_slot:
  end_time: '2019-04-04 11:55:00'
  start_time: '2019-04-04 11:00:00'
session_speakers:
- speaker_bio: Gaurav works in ML SIG as Tech Lead.
  speaker_company: Linaro (Arm)
  speaker_image: /assets/images/speakers/bkk19/gaurav-kaul.jpg
  speaker_location: ''
  speaker_name: Gaurav Kaul
  speaker_position: Tech Lead
  speaker_username: gaurav.kaul
- speaker_bio: ''
  speaker_company: ''
  speaker_image: /assets/images/speakers/bkk19/gaurav-kaul
  speaker_location: ''
  speaker_name: Gaurav Kaul
  speaker_position: ''
  speaker_username: gauravkaul
session_track: Machine Learning/AI
tag: session
tags:
- Tools
- IoT Fog/Gateway/Edge Computing
- Machine Learning/AI
title: Tutorial and Hands-on BKK 2019- Accelerating AI Inference For Edge and IoT
---