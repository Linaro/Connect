---
categories:
- bkk19
description: Although training a Deep Learning network requires significant computing
  resources that is typically done on servers, using a trained network to perform
  inference on edge devices (mobile and IOT) is an effective way to distribute the
  computing burden to reduce latency, and enhance privacy. The rapid growth of inference
  on the edge has led to multiple solutions and significant fragmentation in the ecosystem.
  This talk will provide some background of the workflow when using edge inferencing,
  describe some of the major solutions, and discuss the trade-offs among them.
image:
  featured: 'true'
  path: /assets/images/featured-images/bkk19/BKK19-207.png
session_attendee_num: '9'
session_id: BKK19-207
session_room: Session Room 1 (Lotus 1-2)
session_slot:
  end_time: '2019-04-02 09:25:00'
  start_time: '2019-04-02 09:00:00'
session_speakers:
- speaker_bio: William Bell is a Principal Engineer at Qualcomm, as a member of the
    team commercializing on-device Machine Learning solutions.<br>(Other Bio TBD)
  speaker_company: ''
  speaker_image: /assets/images/speakers/bkk19/william-bell.jpg
  speaker_location: ''
  speaker_name: William Bell
  speaker_position: Principal Engineer, Qualcomm Inc.
  speaker_username: william.bell1
session_track: Machine Learning/AI
tag: session
tags:
- Open Source Development
title: Current Landscape for On-Device Deep Learning Inference
---