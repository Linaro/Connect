---
amazon_s3_presentation_url: https://static.linaro.org/connect/lvc21f/presentations/LVC21F-211.pdf
amazon_s3_video_url: https://static.linaro.org/connect/lvc21f/videos/LVC21F-211.mp4
categories:
- lvc21f
description: "Level: Intermediate \nTiny ONNC is an MLIR-based compiler exporting
  deep neural networks (DNN) into function calls to the ARM CMSIS-NN library. MLIR
  is a high-quality compiler framework addressing software fragmentation issues. By
  supporting variant Intermediate Representations in a single infrastructure, compilers
  can transform variant input languages into a common output form. Tiny ONNC leverages
  the unique power of MLIR to support rich neural network frameworks, including PyTorch,
  Open Neural Network Exchange Format (ONNX), Tensorflow, TensorflowLite, and even
  TVM relay. Tiny ONNC transforms all the input DNN formats into a function composed
  of a series of function calls to the ARM CMSIS-NN library. “One fits all,” MLIR
  makes it possible.\n\nTiny ONNC has enormous optimization approaches, such as automatic
  operator splitting and tensor splitting, addressing on memory constraints of microcontrollers.
  When an operator or a tensor is too big to fit in the cache, Tiny ONNC separates
  the big objects into small pieces and reorganizes networks for reusing the memory.
  Tiny ONNC also supports operators that are not directly supported by CMSIS-NN by
  mathematical equivalent or approximate transformations.\n\nThese optimization approaches
  deliver strong empirical performances while keeping high memory utilization and
  high performance at the same time. On the MLPerf Tiny benchmark, Tiny ONNC achieves
  the same level (<2%) as TensorflowLite for Microcontrollers (TFLM) in terms of performance
  and precision. In the best case, the memory footprint of the generated program is
  only 3/5 of TFLM, and the code size is only 1/10 of TFLM.\n\nIn this talk, we will
  introduce MLIR first, see how it works in Tiny ONNC. And then we will dive into
  memory optimization strategies and approaches. Last, we will explain the experiment
  results to see how Tiny ONNC outperforms its rivals."
image: /assets/images/featured-images/lvc21f/LVC21F-211.png
session_id: LVC21F-211
session_room: Stage 2
session_slot:
  end_time: 09-09-2021 09:55
  start_time: 09-09-2021 09:30
session_speakers:
- speaker_bio: Peter is the co-founder of Skymizer Taiwan Inc. His research interests
    span areas in operating systems, virtualization, and computer architecture. Currently,
    he focuses on topics in AI hardware/software co-design and AI system software,
    including AI compiler and runtime. He was also the maintainer of SkyPat, an open-source
    performance unit-test suite, and one of the maintainers of ARMvisor, one of the
    Kernel-based Virtual Machine solutions on ARM architecture.
  speaker_company: Skymizer
  speaker_image: https://data.pinetool.ai/images/bfdce157-40f4-42aa-9167-e71206232bff.jpeg
  speaker_name: Peter Chang
  speaker_position: Co-founder and Technical Marketing Manager, Skymizer
session_track: IoT and Embedded
tag: session
tags: IoT and Embedded
title: 'LVC21F-211 Tiny ONNC: MLIR-based AI compiler for ARM IoT devices'
---

Level: Intermediate 
Tiny ONNC is an MLIR-based compiler exporting deep neural networks (DNN) into function calls to the ARM CMSIS-NN library. MLIR is a high-quality compiler framework addressing software fragmentation issues. By supporting variant Intermediate Representations in a single infrastructure, compilers can transform variant input languages into a common output form. Tiny ONNC leverages the unique power of MLIR to support rich neural network frameworks, including PyTorch, Open Neural Network Exchange Format (ONNX), Tensorflow, TensorflowLite, and even TVM relay. Tiny ONNC transforms all the input DNN formats into a function composed of a series of function calls to the ARM CMSIS-NN library. “One fits all,” MLIR makes it possible.

Tiny ONNC has enormous optimization approaches, such as automatic operator splitting and tensor splitting, addressing on memory constraints of microcontrollers. When an operator or a tensor is too big to fit in the cache, Tiny ONNC separates the big objects into small pieces and reorganizes networks for reusing the memory. Tiny ONNC also supports operators that are not directly supported by CMSIS-NN by mathematical equivalent or approximate transformations.

These optimization approaches deliver strong empirical performances while keeping high memory utilization and high performance at the same time. On the MLPerf Tiny benchmark, Tiny ONNC achieves the same level (<2%) as TensorflowLite for Microcontrollers (TFLM) in terms of performance and precision. In the best case, the memory footprint of the generated program is only 3/5 of TFLM, and the code size is only 1/10 of TFLM.

In this talk, we will introduce MLIR first, see how it works in Tiny ONNC. And then we will dive into memory optimization strategies and approaches. Last, we will explain the experiment results to see how Tiny ONNC outperforms its rivals.